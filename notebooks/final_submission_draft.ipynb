{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1784fb79",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "# Modelling\n",
    "import pickle\n",
    "# Text cleaning\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from typing import Any, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from rouge import Rouge\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV,\n",
    "                                     StratifiedKFold, train_test_split)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from text_analytics.config import (ACRONYMS, DATA_PATH, RANDOM_STATE,\n",
    "                                   RAW_DATA_PATH, SENTIMENT_CLEANED_DATA_PATH,\n",
    "                                   SUMMARISER_CLEANED_DATA_PATH)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe52a9b",
   "metadata": {},
   "source": [
    "## VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbd5f7",
   "metadata": {},
   "source": [
    "### 1. Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_movies = pd.read_parquet(RAW_DATA_PATH)\n",
    "vader_movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a273d5e",
   "metadata": {},
   "source": [
    "### 2. Preprocessing function helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb555f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lowercase(series: Union[pd.Series, str]) -> Union[pd.Series, str]:\n",
    "    if isinstance(series, str):\n",
    "        return series.lower()\n",
    "    return series.str.lower()\n",
    "\n",
    "\n",
    "def remove_html_tags(series: Union[pd.Series, str]) -> Union[pd.Series, str]:\n",
    "    if isinstance(series, str):\n",
    "        return re.sub(pattern=r\"<.*?>\", repl=\"\", string=series)\n",
    "\n",
    "    return series.str.replace(pat=r\"<.*?>\", repl=\"\", regex=True)\n",
    "\n",
    "\n",
    "def remove_punctuation(word_arr: Union[pd.Series, str]) -> Union[pd.Series, str]:\n",
    "    import string\n",
    "\n",
    "    if isinstance(word_arr, str):\n",
    "        return \" \".join(word for word in word_arr if word not in string.punctuation)\n",
    "\n",
    "    return word_arr.apply(\n",
    "        lambda arr: [word for word in arr if word not in string.punctuation]\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_abbreviations(series: Union[pd.Series, str]) -> Union[pd.Series, str]:\n",
    "\n",
    "    if isinstance(series, str):\n",
    "        return \" \".join(\n",
    "            ACRONYMS.get(word) if word in ACRONYMS.keys() else word\n",
    "            for word in series.split()\n",
    "        )\n",
    "\n",
    "    return series.apply(\n",
    "        lambda sentence: \" \".join(\n",
    "            ACRONYMS.get(word) if word in ACRONYMS.keys() else word\n",
    "            for word in sentence.split()\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_stopwords(\n",
    "    series: Union[pd.Series, str],\n",
    "    stop_words: Union[nltk.corpus.reader.wordlist.WordListCorpusReader, List] = None,\n",
    ") -> Union[pd.Series, str]:\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    if isinstance(series, str):\n",
    "        return \" \".join(word for word in series.split() if word not in stop_words)\n",
    "\n",
    "    return series.apply(\n",
    "        lambda sentence: \" \".join(\n",
    "            word for word in sentence.split() if word not in stop_words\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_words(text: str, tokenizer: str = \"word\") -> npt.ArrayLike:\n",
    "\n",
    "    if tokenizer not in (\"word\", \"sentence\"):\n",
    "        raise ValueError(f\"{tokenizer} must be one of (word, sentence)\")\n",
    "\n",
    "    tokens = {\"word\": word_tokenize(text), \"sentence\": sent_tokenize(text)}\n",
    "    try:\n",
    "        return tokens.get(tokenizer)\n",
    "    except BaseException as err:\n",
    "        print(f\"Unexpected err: {err}, Type: {type(err)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def remove_non_alnum(word_arr: Union[pd.Series, str]) -> Union[pd.Series, str]:\n",
    "    if isinstance(word_arr, str):\n",
    "        return \" \".join(word for word in word_arr.split() if word.isalnum())\n",
    "    return word_arr.apply(lambda arr: [word for word in arr if word.isalnum()])\n",
    "\n",
    "\n",
    "def stemming(word_arr: npt.ArrayLike, stemmer: Any = None) -> npt.ArrayLike:\n",
    "    if stemmer is None:\n",
    "        stemmer = PorterStemmer()\n",
    "    try:\n",
    "        return [stemmer.stem(word) for word in word_arr]\n",
    "    except BaseException as err:\n",
    "        print(f\"Unexpected err: {err}, Type: {type(err)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def lemmatizer(word_arr: npt.ArrayLike, lemmatizer: Any = None) -> npt.ArrayLike:\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        return [lemmatizer.lemmatize(word) for word in word_arr]\n",
    "    except BaseException as err:\n",
    "        print(f\"Unexpected err: {err}, Type: {type(err)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_movies[\"review\"] = remove_html_tags(vader_movies[\"review\"])\n",
    "vader_movies[\"review\"] = convert_lowercase(vader_movies[\"review\"])\n",
    "vader_movies[\"review\"] = convert_abbreviations(vader_movies[\"review\"])\n",
    "vader_movies[\"review\"] = remove_stopwords(vader_movies[\"review\"])\n",
    "vader_movies.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df3a16",
   "metadata": {},
   "source": [
    "### 3. Define a Vader model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaderReviews:\n",
    "    def __init__(self, data: Union[str, pd.DataFrame]) -> None:\n",
    "\n",
    "        try:\n",
    "            self.model = SentimentIntensityAnalyzer()\n",
    "        except BaseException:\n",
    "            nltk.download(\"vader_lexicon\")\n",
    "            self.model = SentimentIntensityAnalyzer()\n",
    "\n",
    "        self.data = data\n",
    "        self.polarity_scores = self.compound_score = self.prediction = None\n",
    "\n",
    "    def calculate_polarity_score(self) -> None:\n",
    "\n",
    "        if isinstance(self.data, str):\n",
    "            self.polarity_scores = self.model.polarity_scores(self.data)\n",
    "        else:\n",
    "            self.polarity_scores = self.data[\"review\"].apply(\n",
    "                lambda sentence: self.model.polarity_scores(sentence)\n",
    "            )\n",
    "\n",
    "    def extract_compound_score(self) -> None:\n",
    "        if isinstance(self.data, str):\n",
    "            self.compound_score = self.polarity_scores.get(\"compound\")\n",
    "        else:\n",
    "            self.compound_score = self.polarity_scores.apply(\n",
    "                lambda score: score.get(\"compound\")\n",
    "            )\n",
    "\n",
    "    def extract_prediction(self) -> None:\n",
    "        if isinstance(self.data, str):\n",
    "            self.prediction = \"positive\" if self.compound_score > 0 else \"negative\"\n",
    "        else:\n",
    "            self.prediction = self.compound_score.apply(\n",
    "                lambda c_score: \"positive\" if c_score > 0 else \"negative\"\n",
    "            )\n",
    "\n",
    "    def return_vader_scores(self) -> None:\n",
    "\n",
    "        if self.polarity_scores is None:\n",
    "            self.calculate_polarity_score()\n",
    "        elif self.compound_score is None:\n",
    "            self.extract_compound_score()\n",
    "        elif self.prediction is None:\n",
    "            self.extract_prediction()\n",
    "        if isinstance(self.data, str):\n",
    "            return (self.compound_score, self.prediction)\n",
    "\n",
    "        self.result = pd.concat(\n",
    "            [self.data, self.compound_score, self.prediction], axis=\"columns\"\n",
    "        )\n",
    "        self.result.columns = [\"review\", \"sentiment\", \"compound_score\", \"prediction\"]\n",
    "\n",
    "        print(self.result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0ecca",
   "metadata": {},
   "source": [
    "### 4. Run predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25121236",
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VaderReviews(data=vader_movies)\n",
    "\n",
    "vr.calculate_polarity_score()\n",
    "vr.extract_compound_score()\n",
    "vr.extract_prediction()\n",
    "vr.return_vader_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ff494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(vr.result[\"sentiment\"], vr.result[\"prediction\"]))\n",
    "print(classification_report(vr.result[\"sentiment\"], vr.result[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0c31d",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba15a09",
   "metadata": {},
   "source": [
    "### 1. Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_raw = pd.read_parquet(RAW_DATA_PATH)\n",
    "movies_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ce860",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15310d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_text_processing(series: pd.Series) -> pd.Series:\n",
    "\n",
    "    series = remove_html_tags(series)\n",
    "    series = convert_lowercase(series)\n",
    "    series = convert_abbreviations(series)\n",
    "    series = remove_stopwords(series)\n",
    "    series = series.str.replace(pat=r\"film|movie|[0-9]+\", repl=\"\", regex=True)\n",
    "    series = series.apply(lambda sentence: tokenize_words(sentence, tokenizer=\"word\"))\n",
    "    series = remove_punctuation(series)\n",
    "    series = series.apply(lambda arr: lemmatizer(arr))\n",
    "    series = remove_non_alnum(series)\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f97d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cleaned = movies_raw.copy(deep=True)\n",
    "movies_cleaned.drop_duplicates(inplace=True)\n",
    "movies_cleaned[\"preprocessed_review\"] = sentiment_text_processing(\n",
    "    series=movies_cleaned[\"review\"]\n",
    ")\n",
    "movies_cleaned[\"preprocessed_review\"] = movies_cleaned[\"preprocessed_review\"].astype(\n",
    "    str\n",
    ")\n",
    "movies_cleaned[\"length\"] = movies_cleaned[\"preprocessed_review\"].apply(len)\n",
    "movies_cleaned[\"class\"] = np.where(movies_cleaned[\"sentiment\"] == \"positive\", 1, 0)\n",
    "movies_cleaned.drop(columns=[\"review\", \"sentiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c03118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    movies_cleaned,\n",
    "    test_size=0.2,\n",
    "    stratify=movies_cleaned[\"class\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d32cb7c",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9ee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbd63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7793fa08",
   "metadata": {},
   "source": [
    "## Extractive Text Summariser "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5eb4b8",
   "metadata": {},
   "source": [
    "### 1. Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_parquet(SUMMARISER_CLEANED_DATA_PATH)\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2a5f5",
   "metadata": {},
   "source": [
    "### 2. Define a ExtractiveTextSummarizer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractiveTextSummarizer:\n",
    "    def __init__(self, article: Union[str, pd.DataFrame]) -> None:\n",
    "        self.article = article\n",
    "        self.frequency_table = defaultdict(int)\n",
    "\n",
    "    def _create_dictionary_table(self, stemmer: Any = None) -> dict:\n",
    "\n",
    "        # removing stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        word_vector = word_tokenize(self.article)\n",
    "\n",
    "        # instantiate the stemmer\n",
    "        if stemmer is None:\n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "        stemmed_word_vector = [stemmer.stem(word) for word in word_vector]\n",
    "        for word in stemmed_word_vector:\n",
    "            if word not in stop_words:\n",
    "                self.frequency_table[word] += 1\n",
    "\n",
    "        return self.frequency_table\n",
    "\n",
    "    def _calculate_sentence_scores(self, sentences: npt.ArrayLike) -> dict:\n",
    "\n",
    "        # algorithm for scoring a sentence by its words\n",
    "        sentence_weights = defaultdict(int)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_wordcount_without_stop_words = 0\n",
    "\n",
    "            for word_weight in self.frequency_table:\n",
    "                sentence_weights[sentence[:7]] += self.frequency_table[word_weight]\n",
    "\n",
    "                if word_weight in sentence.lower():\n",
    "                    sentence_wordcount_without_stop_words += 1\n",
    "\n",
    "            sentence_weights[sentence[:7]] /= sentence_wordcount_without_stop_words\n",
    "\n",
    "        return sentence_weights\n",
    "\n",
    "    def _calculate_threshold_score(self, sentence_weight: dict) -> float:\n",
    "        return np.mean(list(sentence_weight.values()))\n",
    "\n",
    "    def _get_article_summary(\n",
    "        self, sentences: npt.ArrayLike, sentence_weights: dict, threshold: float\n",
    "    ) -> str:\n",
    "        article_summary = [\n",
    "            sentence\n",
    "            for sentence in sentences\n",
    "            if sentence[:7] in sentence_weights\n",
    "            and sentence_weights.get(sentence[:7]) >= threshold\n",
    "        ]\n",
    "\n",
    "        return \" \".join(article_summary)\n",
    "\n",
    "    def run_article_summary(self):\n",
    "\n",
    "        # creating a dictionary for the word frequency table\n",
    "        _ = self._create_dictionary_table()\n",
    "\n",
    "        # tokenizing the sentences\n",
    "        sentences = sent_tokenize(self.article)\n",
    "\n",
    "        # algorithm for scoring a sentence by its words\n",
    "        sentence_scores = self._calculate_sentence_scores(sentences)\n",
    "\n",
    "        # getting the threshold\n",
    "        threshold = self._calculate_threshold_score(sentence_scores)\n",
    "\n",
    "        # producing the summary\n",
    "        article_summary = self._get_article_summary(\n",
    "            sentences, sentence_scores, 0.95 * threshold\n",
    "        )\n",
    "\n",
    "        return article_summary\n",
    "\n",
    "    def get_rouge_score(\n",
    "        self, hypothesis_text: str, reference_text: str\n",
    "    ) -> npt.ArrayLike:\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(hypothesis_text, reference_text)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adefac",
   "metadata": {},
   "source": [
    "### 3. Extract summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88884f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = []\n",
    "articles = movie_reviews.loc[:, \"cleaned_reviews\"].sample(3).values\n",
    "\n",
    "for review in articles:\n",
    "    print(f\"Original Review: \\n{review}\")\n",
    "    print(\"-\" * 200)\n",
    "    extractive_summarizer = ExtractiveTextSummarizer(article=review)\n",
    "    review_summary = extractive_summarizer.run_article_summary()\n",
    "    summary_results.append(review_summary)\n",
    "\n",
    "    print(f\"Summarised Review: \\n{review_summary}\")\n",
    "    print(\"-\" * 200)\n",
    "\n",
    "    # this line is POC for now since we don't have the reference text\n",
    "    print(\n",
    "        extractive_summarizer.get_rouge_score(\n",
    "            hypothesis_text=review_summary, reference_text=review_summary\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c9cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ta22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37e987721a07d9a801a65e99628dc1f05d14dfb697773d267e80d3ef33c8e70f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

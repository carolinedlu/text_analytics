{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key functions to work on to clean / explore data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lowercase\n",
    "\n",
    "\n",
    "def convert_lowercase(arr: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    return arr.str.lower()\n",
    "\n",
    "\n",
    "# remove html tags\n",
    "\n",
    "\n",
    "def remove_html_tags(arr: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    return arr.str.replace(pat=r\"<.*?>\", repl=\"\", regex=True)\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "# remove punctuations\n",
    "def remove_punctuation(arr: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    punctuations = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    return arr.str.translate(punctuations)\n",
    "\n",
    "\n",
    "test_arr = pd.Series([\"this SIUH siu<b><sflsn>.,,,,,,,,,,\", \"ashiuf,,,,, asSIUHF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_html_tags(test_arr)\n",
    "\n",
    "\n",
    "def strip():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"sdfhousd() *((#@\"\n",
    "\n",
    "remove_punctuation(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"g2g\": \"got to go\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"msg\": \"message\",\n",
    "    \"noyb\": \"none of your business\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"wth\": \"what the hell\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "}\n",
    "\n",
    "\n",
    "# abbreviation dictionary\n",
    "def convert_abbreviations(text: str) -> str:\n",
    "    return \" \".join(\n",
    "        acronyms.get(word) if word in acronyms.keys() else word for word in text.split()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    stopwords_english = set(stopwords.words(\"english\"))\n",
    "    return \" \".join(word for word in text.split() if word not in stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_words(text: str) -> npt.ArrayLike:\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "def porter_stemming(word_arr: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in word_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatization function\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def lemmatize(word_arr: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in word_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = [\"snowing\", \"fiery\", \"this\", \"movie\", \"lacking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatize(word_arr))\n",
    "\n",
    "print(porter_stemming(word_arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ta22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37e987721a07d9a801a65e99628dc1f05d14dfb697773d267e80d3ef33c8e70f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

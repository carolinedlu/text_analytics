{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe92f8c",
   "metadata": {},
   "source": [
    "# ISSS609 Text Analytics and Applications\n",
    "## IMBD Movie Review - Extractive Text Summarisation\n",
    "### G1 - Group 4\n",
    "\n",
    "<a id=\"table_of_contents\"></a>\n",
    "### Table of Contents\n",
    "\n",
    "1. [Importing Files and Libraries](#import)\n",
    "2. [Extractive Summarisation steps](#outline)\n",
    "3. [Importing raw data](#setup)\n",
    "4. [Creating a frequency table](#freq)\n",
    "5. [Calculate sentence scores](#ss)\n",
    "6. [Calculate threshold](#threshold)\n",
    "7. [Finetuning threshold](#finetune)\n",
    "8. [Programmatic evaluation](#eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452dba5",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e9d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from text_analytics.config import DATA_PATH\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import List, Any\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# set custom colwidths \n",
    "pd.set_option(\"max_colwidth\", 150)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2f9b4",
   "metadata": {},
   "source": [
    "<a id=\"outline\"></a>\n",
    "### 2. Extractive summarisation steps\n",
    "\n",
    "Here is an outline of steps to build the extractive summariser \n",
    "\n",
    "- Select a raw article to preprocess \n",
    "- Tokenise the sentences to get all stems present \n",
    "- Evaluate the weighted occurrence frequency of the words \n",
    "- Split the paragraph into sentences\n",
    "- Apply the masking threshold to output the summarised review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefaa7c",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "### 3. Importing raw data\n",
    "\n",
    "- A raw article is selected at this stage for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e7765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometim...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;Thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. Thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened ...   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometim...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted ...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />Thi...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. Thi...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_parquet(DATA_PATH / \"imdb_data.parquet\")\n",
    "# preview the dataframe\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e87ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they'd all be \"up\" for this movie.\n"
     ]
    }
   ],
   "source": [
    "article = movie_reviews.loc[5, \"review\"]\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f191493",
   "metadata": {},
   "source": [
    "<a id=\"freq\"></a>\n",
    "### 4. Creating a frequency table\n",
    "\n",
    "- The first step in extractive summarisation is to determine the relative importance of each word within the overall context of the sentence \n",
    "- Every stem in an article's importance can be captured into a frequency table and weighted accordingly\n",
    "- We remove stop words and calculate a frequency table for each article  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6ef10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_table(article, stemmer = None):\n",
    "    #removing stop words\n",
    "    frequency_table = defaultdict(int)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_vector = word_tokenize(article)\n",
    "\n",
    "    # instantiate the stemmer \n",
    "    if stemmer is None: \n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "    stemmed_word_vector = [stemmer.stem(word) for word in word_vector]\n",
    "    for word in stemmed_word_vector:\n",
    "        if word not in stop_words:\n",
    "            frequency_table[word] += 1\n",
    "\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564ae38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table = create_dictionary_table(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74e265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movi</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      frequencies\n",
       ",              11\n",
       ".               6\n",
       "'s              3\n",
       "movi            2\n",
       "''              2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = pd.DataFrame(\n",
    "    {\"frequencies\": frequency_table}\n",
    "    ).sort_values(\"frequencies\", ascending=False)\n",
    "# preview the frequencies\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b536ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movi</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happen</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequencies\n",
       ",                11\n",
       ".                 6\n",
       "'s                3\n",
       "movi              2\n",
       "''                2\n",
       "...             ...\n",
       "happen            1\n",
       "kid               1\n",
       "last              1\n",
       "like              1\n",
       "year              1\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47745c",
   "metadata": {},
   "source": [
    "<a id=\"ss\"></a>\n",
    "### 5. Calculate sentence scores\n",
    "\n",
    "- The frequency scores are used to determine the importance of each sentence \n",
    "- For instance, `I like this movie` will return a score of 3 if `like=1` and `movie=2`\n",
    "- To ensure long sentences do not dominate shorter sentences, we normalise the scores of each sentence by dividing each sentence score by its word length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34078b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc75488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_scores(sentences: npt.ArrayLike, frequency_table: dict) -> dict:   \n",
    "    # Every sentence is scored by how important its constituent words are in the frequency table\n",
    "    sentence_weights = defaultdict(int)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "\n",
    "        for word_weight in frequency_table:\n",
    "            sentence_weights[sentence[:7]] += frequency_table[word_weight]\n",
    "\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "\n",
    "        sentence_weights[sentence[:7]] /= sentence_wordcount_without_stop_words\n",
    "\n",
    "    return sentence_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ae13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_weights = calculate_sentence_scores(sentences, frequency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b81660dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>If I ha</th>\n",
       "      <td>10.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And the</th>\n",
       "      <td>7.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It just</th>\n",
       "      <td>7.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The kid</th>\n",
       "      <td>6.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Probabl</th>\n",
       "      <td>6.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_weights\n",
       "If I ha         10.625000\n",
       "And the          7.727273\n",
       "It just          7.083333\n",
       "The kid          6.538462\n",
       "Probabl          6.071429"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_weight_preview = pd.DataFrame(\n",
    "    {\"sentence_weights\": sentence_weights}\n",
    "    ).sort_values(\"sentence_weights\", ascending=False)\n",
    "sentence_weight_preview.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af700a85",
   "metadata": {},
   "source": [
    "<a id=\"threshold\"></a>\n",
    "### 6. Calculate the threshold for a token to be counted as important \n",
    "\n",
    "- We can adjust the threshold by multiplying the mean of the scores by an alpha value\n",
    "- Alternatives, such as the median, can also be used to compute the threshold for inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d280d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold_score(sentence_weights: dict, alpha: float = 1.0) -> float:\n",
    "    return np.mean(list(sentence_weights.values())) * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07abb8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold weight for example article: 7.29\n"
     ]
    }
   ],
   "source": [
    "print(f\"Threshold weight for example article: {calculate_threshold_score(sentence_weights):.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52555d3",
   "metadata": {},
   "source": [
    "<a id=\"finetune\"></a>\n",
    "### 7. Finetuning the threshold \n",
    "\n",
    "- Different threshold values represent a trade-off between comprehension and length\n",
    "- Lower thresholds result in longer sentences, but will contain more contextual markers\n",
    "- The optimal threshold can either be determined manually or programmatically via a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06727bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_summary(sentences: npt.ArrayLike, sentence_weights: dict, threshold: float) -> str:\n",
    "    article_summary = [sentence for sentence in sentences if sentence[:7] in sentence_weights and sentence_weights.get(sentence[:7]) >= threshold]\n",
    "    return \" \".join(article_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4cd855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95, 1.05, 1.15, 1.25])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_values = np.arange(0.95, 1.25, 0.1)\n",
    "alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9bc1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At threshold of: 0.95\n",
      "Result: It just never gets old, despite my having seen it some 15 or more times in the last 25 years. And the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they'd all be \"up\" for this movie.\n",
      "At threshold of: 1.05\n",
      "Result: And the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they'd all be \"up\" for this movie.\n",
      "At threshold of: 1.15\n",
      "Result: If I had a dozen thumbs, they'd all be \"up\" for this movie.\n",
      "At threshold of: 1.25\n",
      "Result: If I had a dozen thumbs, they'd all be \"up\" for this movie.\n"
     ]
    }
   ],
   "source": [
    "for alpha in alpha_values: \n",
    "    threshold_score = calculate_threshold_score(sentence_weights=sentence_weights, alpha=alpha) \n",
    "    final_result = get_article_summary(sentences=sentences, sentence_weights=sentence_weights, threshold=threshold_score) \n",
    "\n",
    "    print(f\"At threshold of: {alpha:.02f}\")\n",
    "    print(f\"Result: {final_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed142f6",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "\n",
    "### 8. Programmatic evaluation\n",
    "\n",
    "- We wrap up all the previous steps into an `ExtractiveTextSummarizer` class \n",
    "- To evaluate the effectiveness of the summarisation at different thresholds, we have manually summarised 101 movie reviews \n",
    "- Our human-labelled summary serves as a reference to estimate the algorithm's ability to pick out important aspects of the review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7226c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_movie_reviews = pd.read_csv(DATA_PATH / \"review_evaluation.csv\", index_col=0).iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77c3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractiveTextSummarizer:\n",
    "    def __init__(self, article: str, alpha: float = 1.0) -> None:\n",
    "        self.article = article\n",
    "        self.alpha = alpha         \n",
    "        self.frequency_table = defaultdict(int)\n",
    "\n",
    "    def _create_dictionary_table(self, stemmer: Any = None) -> dict:\n",
    "   \n",
    "        #removing stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        word_vector = word_tokenize(self.article)\n",
    "\n",
    "        # instantiate the stemmer \n",
    "        if stemmer is None: \n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "        stemmed_word_vector = [stemmer.stem(word) for word in word_vector]\n",
    "        for word in stemmed_word_vector:\n",
    "            if word not in stop_words:\n",
    "                self.frequency_table[word] += 1\n",
    "\n",
    "        return self.frequency_table\n",
    "\n",
    "\n",
    "    def _calculate_sentence_scores(self, sentences: npt.ArrayLike) -> dict:   \n",
    "\n",
    "        #algorithm for scoring a sentence by its words\n",
    "        sentence_weights = defaultdict(int)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_wordcount_without_stop_words = 0\n",
    "\n",
    "            for word_weight in self.frequency_table:\n",
    "                sentence_weights[sentence[:7]] += self.frequency_table[word_weight]\n",
    "\n",
    "                if word_weight in sentence.lower():\n",
    "                    sentence_wordcount_without_stop_words += 1\n",
    "\n",
    "            sentence_weights[sentence[:7]] /= sentence_wordcount_without_stop_words\n",
    "\n",
    "        return sentence_weights\n",
    "\n",
    "\n",
    "    def _calculate_threshold_score(self, sentence_weight: dict) -> float:\n",
    "        return np.mean(list(sentence_weight.values())) * self.alpha\n",
    "\n",
    "\n",
    "    def _get_article_summary(self, sentences: npt.ArrayLike, sentence_weights: dict, threshold: float) -> str:\n",
    "        article_summary = [sentence for sentence in sentences if sentence[:7] in sentence_weights and sentence_weights.get(sentence[:7]) >= threshold]\n",
    "\n",
    "        return \" \".join(article_summary)\n",
    "\n",
    "    def run_article_summary(self):\n",
    "\n",
    "        #creating a dictionary for the word frequency table\n",
    "        _ = self._create_dictionary_table()\n",
    "\n",
    "        #tokenizing the sentences\n",
    "        sentences = sent_tokenize(self.article)\n",
    "\n",
    "        #algorithm for scoring a sentence by its words\n",
    "        sentence_scores = self._calculate_sentence_scores(sentences)\n",
    "\n",
    "        # getting the threshold\n",
    "        threshold = self._calculate_threshold_score(sentence_scores)\n",
    "\n",
    "        #producing the summary\n",
    "        article_summary = self._get_article_summary(sentences, sentence_scores, threshold)\n",
    "\n",
    "        return article_summary\n",
    "\n",
    "    def get_rouge_score(self, hypothesis_text: str, reference_text: str) -> npt.ArrayLike:\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(hypothesis_text, reference_text)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a324b",
   "metadata": {},
   "source": [
    "- We store the results the ROUGE-1 F1 of each summarisation under their respective alpha thresholds and calculate the mean values of 10 randomly chosen articles\n",
    "- Given that we consider 20% of the original length to be an acceptable amount, we select 0.95 as the final threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20d0ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproducibility\n",
    "random.seed(2022)\n",
    "random_subset = random.sample(range(101), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef4b1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = defaultdict(list)\n",
    "percentage_summarised = defaultdict(list)\n",
    "\n",
    "for idx, article in enumerate(labelled_movie_reviews.loc[random_subset, \"review\"].values):\n",
    "    ext = ExtractiveTextSummarizer(article=article)\n",
    "    original_article_length = len(article.split())\n",
    "    for alpha in alpha_values: \n",
    "        ext.alpha = alpha\n",
    "        article_summary = ext.run_article_summary() \n",
    "        rouge_score = ext.get_rouge_score(\n",
    "            hypothesis_text=article_summary, \n",
    "            reference_text=labelled_movie_reviews.loc[idx, \"Summary\"])\n",
    "\n",
    "        _, _, f1 = rouge_score[0].get(\"rouge-1\").values()\n",
    "\n",
    "        percentage_summarised[alpha].append(len(article_summary.split()) / original_article_length)\n",
    "        result[alpha].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10f3976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha value of 0.95\n",
      "Score: 0.123\n",
      "Percentage summarised: 22.75%\n",
      "Alpha value of 1.05\n",
      "Score: 0.119\n",
      "Percentage summarised: 21.79%\n",
      "Alpha value of 1.15\n",
      "Score: 0.110\n",
      "Percentage summarised: 16.39%\n",
      "Alpha value of 1.25\n",
      "Score: 0.083\n",
      "Percentage summarised: 11.54%\n"
     ]
    }
   ],
   "source": [
    "final_scores = zip(\n",
    "    alpha_values, \n",
    "    map(np.mean, result.values()),\n",
    "    map(np.mean, percentage_summarised.values())\n",
    "    )\n",
    "\n",
    "for alpha, score, percentage in final_scores: \n",
    "    print(f\"Alpha value of {alpha:.02f}\")\n",
    "    print(f\"Score: {score:.03f}\")\n",
    "    print(f\"Percentage summarised: {percentage:.02%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ta22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37e987721a07d9a801a65e99628dc1f05d14dfb697773d267e80d3ef33c8e70f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

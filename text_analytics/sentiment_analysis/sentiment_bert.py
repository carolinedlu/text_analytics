# -*- coding: utf-8 -*-
"""Sentiment_Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DU7E3hIr_WKB1yOKUZNoL9e5Bm92q6Nj
"""

!pip install transformers

import pandas as pd
import numpy as np
import transformers
from keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tokenizers import BertWordPieceTokenizer
import warnings; warnings.filterwarnings('ignore')

df_raw = pd.read_csv('/content/drive/MyDrive/ISSS609/IMDB Dataset01_clean.csv') # test
df_raw.head()

class SentimentBert:
  def __init__(self):
    self.model = None
    # First load the real tokenizer
    self.tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True) # bert-base
    # Save the loaded tokenizer locally
    self.tokenizer.save_pretrained('.')
    # Reload it with the huggingface tokenizers library
    self.fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)
    self.bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')

    # get_data
    df = pd.read_csv('/content/drive/MyDrive/ISSS609/IMDB Dataset_clean.csv')
    x = df['clean']
    y = df['label']
    self.x_train_orig, self.x_test_orig, self.y_train, self.y_test = train_test_split(x, y, test_size=0.2, stratify = y, random_state=2022)
    self.x_train = self.fast_encode(self.x_train_orig.values, self.fast_tokenizer, maxlen=400)
    self.x_test = self.fast_encode(self.x_test_orig.values, self.fast_tokenizer, maxlen=400)
  
  def fast_encode(self, texts, tokenizer, chunk_size=256, maxlen=400):
    tokenizer.enable_truncation(max_length=maxlen)
    tokenizer.enable_padding(length=maxlen)
    all_ids = []
    
    for i in range(0, len(texts), chunk_size):
        text_chunk = texts[i:i+chunk_size].tolist()
        encs = tokenizer.encode_batch(text_chunk)
        all_ids.extend([enc.ids for enc in encs])
    
    return np.array(all_ids)

  def build_model(self, transformer, max_len=400):
    
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    out = Dense(1, activation='sigmoid')(cls_token)
    
    model = Model(inputs=input_word_ids, outputs=out)
    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])
    
    return model
  def load_model(self):
    self.model = load_model('/content/drive/MyDrive/ISSS609/bert_base_01.h5', custom_objects={'TFDistilBertModel': self.bert_model})

  def evaluate(self):
    print("Accuracy of the model on Testing Data is - " , self.model.evaluate(self.x_test, self.y_test)[1]*100 , "%")
    pred = self.model.predict(self.x_test)
    pred = np.round(pred).astype(int)
    print(classification_report(self.y_test, pred, target_names = ['Negative','Positive']))

  def test_single(self, review):
    review = self.fast_encode(np.array([review]), self.fast_tokenizer, maxlen=400)
    prediction = np.round(self.model.predict(review)).astype(int)
    if prediction == 0 : return 'Negative'
    return 'Positive'
if __name__ == '__main__':
    sentiment_bert = SentimentBert()
    sentiment_bert.load_model()
    #sentiment_bert.evaluate() need GPU, CPU take too long time

    record = df_raw.sample(1)
    article = record['REVIEW'].values[0]
    rating = record['RATING'].values[0]
    print('Original Review: ')
    print(article)
    print('====================================================')
    print('RATING:', rating)
    print('====================================================')
    print(sentiment_bert.test_single(article))
    '''
    summaraizer = Ext_text_summarizer(article)
    result = summaraizer._run_article_summary(summaraizer.para)
    print(result)
    print('====================================================')
    print(summaraizer._get_rouge_score(article, result))
    '''

